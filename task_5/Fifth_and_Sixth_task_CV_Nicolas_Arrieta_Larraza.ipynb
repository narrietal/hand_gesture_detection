{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fifth_and_Sixth_task_CV_Nicolas_Arrieta_Larraza.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOYuEwdOqKH6KWiGTLX0vc1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AucZk2sHQ89q"},"source":["# README\n","\n","This delivery contains the implementation of the 5th and 6th tasks of the \"Automatic Signal Detector\" project of the Computer Vision course.\n","In order to run succesfully, one must download the harcascade from \n","\n","*   https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n","\n","Import them in their own drive and change the path in the \"Synchronizing drive files section\"\n","\n","After running the whole notebook the Dataset is created in Drive. Please, find my personal dataset in the following link:\n","[Dataset of Nicolas Arrieta Larraza](https://drive.google.com/drive/folders/1tUy_8Kvx9Qsi_RlgrltXPqUpey3o_n3f?usp=sharing)\n","\n","The Dataset follows the structure shown below:\n","\n","\n","*   A\n","  *  A_32\n","      *  0.jpg \n","      *  1.jpg...\n","  *  A_224\n","      *  0.jpg \n","      *  1.jpg...\n","*   B ...\n","\n","\n","## Important notes:\n","\n","\n","1.   The cells that use the webcam run in a loop, one has to **stop manually the cell** and **re-run the notebook from the next cell** to run the rest of the notebook successfully."]},{"cell_type":"markdown","metadata":{"id":"wsSGUlNySBqt"},"source":["# TASK 5 & 6 - MACHINE LEARNING AND COMPUTER VISION PROJECT\n","\n","Author: Nicolás Arrieta Larraza\n","\n","Date: 15/03/2021"]},{"cell_type":"markdown","metadata":{"id":"UoYbeomsSSkg"},"source":["## Initializing"]},{"cell_type":"markdown","metadata":{"id":"3zIBhpYWSVsj"},"source":["### Importing libraries"]},{"cell_type":"code","metadata":{"id":"2P1YxUFGz-jZ"},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import io\n","import cv2 # OpenCV library\n","from pathlib import Path\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"hzYH3_Zv0-lP","executionInfo":{"status":"ok","timestamp":1617566869525,"user_tz":-120,"elapsed":2416,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"ae16fc90-87a9-48f6-e24a-dcd5a1f6c099"},"source":["cv2.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4.1.2'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"rj9EYEB6SaBX"},"source":["### Synchronizing Drive files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmAJFXZ11SKp","executionInfo":{"status":"ok","timestamp":1617567097420,"user_tz":-120,"elapsed":230301,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"3b18d2ee-6cc8-4eb1-edcf-fdb2fb4a234b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xAUO8HAA1Svj"},"source":["# Change for custom location\n","face_cascade_path = \"/content/drive/MyDrive/UCA/ComputerVision/task_1/haarcascade_frontalface_default.xml\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbG19z_8m4UA"},"source":["Path to custom signal dataset"]},{"cell_type":"code","metadata":{"id":"jnrzSY2Dmvgo"},"source":["dataset_path = \"/content/drive/MyDrive/UCA/ComputerVision/task_5/SignalDataset/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dxou5JMRTe0z"},"source":["### Importing the cascade filter to OpenCV"]},{"cell_type":"code","metadata":{"id":"DG1aNSzj1N5N"},"source":["face_cascades = cv2.CascadeClassifier(face_cascade_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-6q96FxStUX"},"source":["## Defining functions"]},{"cell_type":"markdown","metadata":{"id":"VkpGymXDS2qs"},"source":["Function to capture video from computer webcam trough the web browser (given)"]},{"cell_type":"code","metadata":{"id":"G3XFOh6U1IgL"},"source":["from google.colab.output import eval_js\n","\n","def VideoCapture():\n","  js = Javascript('''\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  ''')\n","  display(js)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7OL34_US5W3"},"source":["Functions to convert base64 to bytes array format and viceversa (given). These functions are necessary since the images in Javascript work with base64 format and OpenCV works with bytes array."]},{"cell_type":"code","metadata":{"id":"yOJXhrX41K76"},"source":["def b64_to_bytes(byte):\n","  jpeg = b64decode(byte.split(',')[1])\n","  im = Image.open(io.BytesIO(jpeg))\n","  return np.array(im)\n","\n","def bytes_to_b64(image):\n","  image = Image.fromarray(image)\n","  buffer = io.BytesIO()\n","  image.save(buffer, 'jpeg')\n","  buffer.seek(0)\n","  x = b64encode(buffer.read()).decode('utf-8')\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-hc2iMTS79u"},"source":["Function to plot histogram of colors"]},{"cell_type":"code","metadata":{"id":"bbhC3TES1NPF"},"source":["def show_hist(hist):\n","  bin_count = hist.shape[0]\n","  bin_w = 24\n","  img = np.zeros((256, bin_count*bin_w, 3), np.uint8)\n","  for i in range(bin_count):\n","      h = int(hist[i])\n","      cv2.rectangle(img, (i*bin_w+2, 255), ((i+1)*bin_w-2, 255-h), (int(180.0*i/bin_count), 255, 255), -1)\n","  img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n","  cv2_imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OHqLRyYMTGjj"},"source":["Function to compute face recognition"]},{"cell_type":"code","metadata":{"id":"xBdlZAaT1oCm"},"source":["def detect_faces(img, cascades):\n","  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  faces = cascades.detectMultiScale(gray, 1.3, 4)\n","  return faces"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkG2P03Awn6v"},"source":["Function to scale a rotated box by a factor. It is used to scale up the bounding box to remove the noise in the edges of the rectangle."]},{"cell_type":"code","metadata":{"id":"NojYjRfWwlgW"},"source":["def scale_contour(pts, scale_x, scale_y):\n","    M = cv2.moments(pts)\n","\n","    if M['m00'] == 0:\n","      return pts\n","\n","    cx = int(M['m10']/M['m00'])\n","    cy = int(M['m01']/M['m00'])\n","\n","    cnt_norm = pts - [cx, cy]\n","    cnt_scaled = cnt_norm * np.array([scale_x, scale_y])\n","    cnt_scaled = cnt_scaled + [cx, cy]\n","    cnt_scaled = cnt_scaled.astype(np.int32)\n","\n","    return cnt_scaled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24l3DKy4n5WC"},"source":["Function to create box arounf CAM Shift resulting box\n"]},{"cell_type":"code","metadata":{"id":"TI5ajMl_n48Q"},"source":["def get_hand_box(pts, im_w, im_h):\n","  x_top_l = max(0, min(pts[:,0]))\n","  y_top_l = max(0, min(pts[:,1]))\n","  x_bottom_r = min(im_w, max(pts[:,0]))\n","  y_bottom_r = min(im_h,  max(pts[:,1 ]))\n","  return (x_top_l, y_top_l), (x_bottom_r, y_bottom_r)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0gFldn3QrC2"},"source":["Function to resize and save images in Drive "]},{"cell_type":"code","metadata":{"id":"wN7_LPdnQquo"},"source":["def save_img(res_dim, count, letter, im, hand_box):\n","  hand = cv2.cvtColor(im[hand_box[0][1]:hand_box[1][1], hand_box[0][0]:hand_box[1][0]], cv2.COLOR_BGR2GRAY)\n","  hand = cv2.resize(hand, (res_dim,res_dim))\n","  path = dataset_path+letter+'/'+letter+'_'+str(res_dim)+'/'\n","  cv2.imwrite(path + str(count) + '.jpg', hand)\n","  return 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pxu7T3FghdHJ"},"source":["Function to flip images of the dataset. It is a way of of applying data augmentation to increase our dataset."]},{"cell_type":"code","metadata":{"id":"ptlGrr_2hbd1"},"source":["def flip_images(path):\n","  for file in os.listdir(path):\n","    number = file.split(\".\")[0]\n","    im = Image.open(path+file)\n","    im_t = im.transpose(Image.FLIP_LEFT_RIGHT)\n","    im_t.save(path+number+'_flipped'+'.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpgaX6L7TYmN"},"source":["## Capturing region of the recognized faced"]},{"cell_type":"markdown","metadata":{"id":"GbmfgAerXeko"},"source":["First, we need to capture a frame of a face in order to compute it's histogram"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":760},"id":"1ZTTYdvv1qyU","executionInfo":{"status":"ok","timestamp":1617442761480,"user_tz":-120,"elapsed":12710,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"fac2bdca-88d3-4dd6-a92e-7cca6a5b1a1b"},"source":["VideoCapture()\n","eval_js('create()')\n","\n","while True:\n","  response = input(\"Type anything when ready!:\")\n","\n","  # Detecting faces\n","  byte = eval_js('capture()')\n","  im = b64_to_bytes(byte)\n","  im_copy = im.copy()\n","  faces = detect_faces(im, face_cascades)\n","\n","  # Drawing a green rectangle in face's region\n","  if len(faces) == 1:\n","    face = faces[0]\n","    cv2.rectangle(im_copy,(face[0],face[1]),(face[0] + face[2], face[1] + face[3]),(0,255,0),2)\n","  else:\n","    continue\n","  # Showing capture\n","  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im_copy)))\n","  response = input(\"Use this bounding box? [y or n]:\")\n","  # We offer the user the option to keep the previous frame or redo it\n","  if response == 'y':\n","    # In case of accepting we show the selected face frame\n","    frame = im[face[1]:face[1]+face[3], face[0]:face[0]+face[2]]\n","    eval_js('showimg(\"{}\")'.format(bytes_to_b64(frame)))\n","    # Asign it to the tracking window variable that will be use in next steps\n","    tracking_window_face = face\n","    break\n","\n","im = im"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Type anything when ready!:a\n","Use this bounding box? [y or n]:y\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B7J5drGFT0Fb"},"source":["## Building histogram from detected face"]},{"cell_type":"markdown","metadata":{"id":"HGR3ZrGBXob7"},"source":["We compute the histogram of color (only HUE channel) of the face region in order to \"back project\" it in the entire region. Then we check fro every pixel in the image the probability of its color to be part of the histogram. Therefore, the parts of the body with similar colors will show also high probability like arms, hands, etc."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"myNffKnQ1tGG","executionInfo":{"status":"ok","timestamp":1617442771086,"user_tz":-120,"elapsed":619,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"cea4b20c-3773-47c6-cca7-7ab6cacaad12"},"source":["# Transform the previous frame into HSV\n","hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","# Creating an inRange mask for pixels\n","mask = cv2.inRange(hsv,np.array((0,64,32)), np.array((180,200,200)))\n","# Compute the histogram of the frame (use only the HUE channel). See `https://bit.ly/3pdVUEd`\n","# Take into account only pixels which are not too bright and not too dark (use the previous mask)\n","# Use 16 bins and speicfy the range of the hue ([0, 180])\n","hist = cv2.calcHist([hsv],[0], mask, [16], [0,180])\n","# Normalize the histogram between 0 (lowest intensity) and 255 (highest intensity) (use MinMax normalization `cv.NORM_MINMAX`) using the method `https://bit.ly/3jMGhCj`\n","hist = cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n","# Reashape the histogram into a 1-D array (use `.reshape(-1)`)\n","hist = hist.reshape(-1)\n","# Show the histogram\n","show_hist(hist)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAIAAAA1FTYwAAAD5UlEQVR4nO3WMWpUURSA4fdCNhGsxGVIKnEZYiUuI2QZYiUuQ1IFlxGsxF340uQRmxjCGfLL+H3VNPdwZpj7c9cFjs+bbXT8aj3QHjzipF4A+H8JEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjKn9QLwhy/b6Pj79UB78Ey8gICMFxA86GwZvch+LV5kj/ACAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkTusF4PhdLNvk+OWyHmqTf40AcQi/RxdsOTnaC8bf3QVo9ve57/P2bjbn6z7n22zO27sPP2df7MX+xS5mcy73OS9nc37c/9CvR4PW7/ucT7M5H0fHeaKb2U19td/UbbmezFmX833Oh9mcz7dxKSEKMHNIpwAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=RGB size=384x256 at 0x7F8432B74410>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"bbk6wyuxUAOs"},"source":["## Erasing face detection, detecting hand and creating a dataset"]},{"cell_type":"markdown","metadata":{"id":"5ech4nwj3yyI"},"source":["In the code below we remove the face detection to focus only on the hand detection. In order to do it we are gonna apply the CAM Shift algorithm to detect the face and erase it by setting probability 0 in the whole bounding box area of the face. Once done, we use CAM Shift again to detect the hand (now it will be the section with more density of probability in the image) and plot a bounding box around it.\n","\n","Once we detect the image, we build a dataset of three letters. For that we crop the hand images, resize them into two different types of dimensions 32x32 and 224x224 and save them in Drive.\n","\n","It is worth mentioning that this system does not work very well with people with beard since the colors of the beard are dominant in the histogram"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RQ3Wn5bvz0yn","executionInfo":{"status":"error","timestamp":1617447040814,"user_tz":-120,"elapsed":246205,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"ad57e957-7f4b-40db-a94b-31411891fcb0"},"source":["\n","VideoCapture()\n","eval_js('create()')\n","# Stop the mean-shift algorithm iff we effectuated 10 iterations or the computed mean does not change by more than 1pt ~ 1.3px in both directions\n","term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n","\n","byte = eval_js('capture()')\n","im_width, im_height = im.shape[1],im.shape[0]\n","im = b64_to_bytes(byte)\n","\n","tracking_window_hand = (0,0,im_width,im_height) # Define the initial tracking window for the hand. It spans the entire caption\n","already_defined_box = False # Auxiliar variable that checks if the hand box is well defined\n","done_A, done_B, done_C = False, False, False # Auxiliar variables to manage the creation of the dataset\n","img_count = 0 # Auxiliar variable to count the images created\n","\n","while True:\n","  byte = eval_js('capture()')\n","  im = b64_to_bytes(byte)\n","  # Transform the image into HSV\n","  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","  \n","  # Compute the standard not too bright not too dark mask\n","  mask = cv2.inRange(hsv, np.array((0., 64., 32.)), np.array((180., 200., 200.)))\n","\n","  # Back project the histogramm on the hsv img\n","  prob = cv2.calcBackProject([hsv],[0], hist, [0,180], scale=1)\n","\n","  # Apply the mask\n","  prob = prob & mask\n","\n","  # First look up for the face using cam shift starting from `tracking_window_face`\n","  (x,y,w,h) = tracking_window_face\n","  bbox, tracking_window_face = cv2.CamShift(prob, tracking_window_face, term_crit)\n","\n","  # Retrieve the rotated bounding rectangle\n","  pts = cv2.boxPoints(bbox).astype(np.int)\n","\n","  # Scale the rotated bounding box 1.5x times to eliminate detected parts outside the box \n","  scaled_pts = scale_contour(pts, 1.5, 1.5)\n","\n","  # Fill the rotated face bounding box with 0 in the prob map using`cv2.fillPoly`\n","  cv2.fillPoly(prob, [scaled_pts], 0)\n","\n","  # Draw the boundix box around the face\n","  cv2.polylines(im, [scaled_pts], True, (255, 255 , 0), 2)\n","\n","  # Now look up for the hand using cam shift starting from `tracking_window_hand`\n","  bbox, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n","\n","  pts = cv2.boxPoints(bbox).astype(np.int)\n","\n","  # Scale the contour around the hand\n","  pts = scale_contour(pts, 1.5, 1.5)\n","\n","  #Creating box around cam shift leaned box\n","  hand_box = get_hand_box(pts, im_width, im_height)\n","\n","  cv2.rectangle(im, hand_box[0], hand_box[1], (255,255,0), 2)\n","\n","  #Checks if the hand box is in the right positiom before taking samples, needs human interaction\n","  if not already_defined_box:\n","    eval_js('showimg(\"{}\")'.format(bytes_to_b64(im)))\n","    response = input(\"Is the hand box right positioned? [y or n]:\")\n","    if response == 'y':\n","      already_defined_box = True\n","\n","  #Saving images for each letter and image dimension.\n","  #We decided to create up to 150 images since we can duplicate the number by applying data augmentation in the next step\n","  elif not done_A:\n","    save_img(32, img_count, 'A', im, hand_box)\n","    img_count += save_img(224, img_count, 'A', im, hand_box)\n","    print('images A saved:',img_count)\n","    if img_count == 150:\n","      already_defined_box = False\n","      done_A = True\n","      img_count = 0\n","  elif not done_B:\n","    save_img(32, img_count, 'B', im, hand_box)\n","    img_count += save_img(224, img_count, 'B', im, hand_box)\n","    print('images B saved:',img_count)\n","    if img_count == 150:\n","      already_defined_box = False\n","      done_B = True\n","      img_count = 0\n","  elif not done_C:\n","    save_img(32, img_count, 'C', im, hand_box)\n","    img_count += save_img(224, img_count, 'C', im, hand_box)\n","    print('images C saved:',img_count)\n","    if img_count == 150:\n","      done_C = True\n","  else:\n","    print(\"C'est fini\")\n","\n","  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im)))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Is the hand box right positioned? [y or n]:\n","Is the hand box right positioned? [y or n]:\n","Is the hand box right positioned? [y or n]:y\n","images B saved: 1\n","images B saved: 2\n","images B saved: 3\n","images B saved: 4\n","images B saved: 5\n","images B saved: 6\n","images B saved: 7\n","images B saved: 8\n","images B saved: 9\n","images B saved: 10\n","images B saved: 11\n","images B saved: 12\n","images B saved: 13\n","images B saved: 14\n","images B saved: 15\n","images B saved: 16\n","images B saved: 17\n","images B saved: 18\n","images B saved: 19\n","images B saved: 20\n","images B saved: 21\n","images B saved: 22\n","images B saved: 23\n","images B saved: 24\n","images B saved: 25\n","images B saved: 26\n","images B saved: 27\n","images B saved: 28\n","images B saved: 29\n","images B saved: 30\n","images B saved: 31\n","images B saved: 32\n","images B saved: 33\n","images B saved: 34\n","images B saved: 35\n","images B saved: 36\n","images B saved: 37\n","images B saved: 38\n","images B saved: 39\n","images B saved: 40\n","images B saved: 41\n","images B saved: 42\n","images B saved: 43\n","images B saved: 44\n","images B saved: 45\n","images B saved: 46\n","images B saved: 47\n","images B saved: 48\n","images B saved: 49\n","images B saved: 50\n","images B saved: 51\n","images B saved: 52\n","images B saved: 53\n","images B saved: 54\n","images B saved: 55\n","images B saved: 56\n","images B saved: 57\n","images B saved: 58\n","images B saved: 59\n","images B saved: 60\n","images B saved: 61\n","images B saved: 62\n","images B saved: 63\n","images B saved: 64\n","images B saved: 65\n","images B saved: 66\n","images B saved: 67\n","images B saved: 68\n","images B saved: 69\n","images B saved: 70\n","images B saved: 71\n","images B saved: 72\n","images B saved: 73\n","images B saved: 74\n","images B saved: 75\n","images B saved: 76\n","images B saved: 77\n","images B saved: 78\n","images B saved: 79\n","images B saved: 80\n","images B saved: 81\n","images B saved: 82\n","images B saved: 83\n","images B saved: 84\n","images B saved: 85\n","images B saved: 86\n","images B saved: 87\n","images B saved: 88\n","images B saved: 89\n","images B saved: 90\n","images B saved: 91\n","images B saved: 92\n","images B saved: 93\n","images B saved: 94\n","images B saved: 95\n","images B saved: 96\n","images B saved: 97\n","images B saved: 98\n","images B saved: 99\n","images B saved: 100\n","images B saved: 101\n","images B saved: 102\n","images B saved: 103\n","images B saved: 104\n","images B saved: 105\n","images B saved: 106\n","images B saved: 107\n","images B saved: 108\n","images B saved: 109\n","images B saved: 110\n","images B saved: 111\n","images B saved: 112\n","images B saved: 113\n","images B saved: 114\n","images B saved: 115\n","images B saved: 116\n","images B saved: 117\n","images B saved: 118\n","images B saved: 119\n","images B saved: 120\n","images B saved: 121\n","images B saved: 122\n","images B saved: 123\n","images B saved: 124\n","images B saved: 125\n","images B saved: 126\n","images B saved: 127\n","images B saved: 128\n","images B saved: 129\n","images B saved: 130\n","images B saved: 131\n","images B saved: 132\n","images B saved: 133\n","images B saved: 134\n","images B saved: 135\n","images B saved: 136\n","images B saved: 137\n","images B saved: 138\n","images B saved: 139\n","images B saved: 140\n","images B saved: 141\n","images B saved: 142\n","images B saved: 143\n","images B saved: 144\n","images B saved: 145\n","images B saved: 146\n","images B saved: 147\n","images B saved: 148\n","images B saved: 149\n","images B saved: 150\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-76832fceda21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malready_defined_box\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_to_b64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is the hand box right positioned? [y or n]:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0malready_defined_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"HfUXUWi9hp1z"},"source":["### Increasing Dataset by applying data augmentation"]},{"cell_type":"markdown","metadata":{"id":"47JouxRJh6Xz"},"source":["In the following cell we will flip all the images of our dataset to increase its size."]},{"cell_type":"code","metadata":{"id":"OeVfNFdat2RN"},"source":["letters = ['A','B','C']\n","dimensions = ['32','224']\n","for letter in letters:\n","  for dim in dimensions:\n","    path = dataset_path+letter+'/'+letter+'_'+dim+'/'\n","    flip_images(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sLtXAmM2ptJ_"},"source":["## Access to Dataset"]},{"cell_type":"markdown","metadata":{"id":"F69AjQMVpz2F"},"source":["After running the whole notebook the Dataset is created in Drive. Please, find my personal dataset in the following link:\n","[Dataset of Nicolas Arrieta Larraza](https://drive.google.com/drive/folders/1tUy_8Kvx9Qsi_RlgrltXPqUpey3o_n3f?usp=sharing)\n","\n","The Dataset follows the structure shown below:\n","\n","\n","*   A\n","  *  A_32\n","      *  0.jpg \n","      *  1.jpg...\n","  *  A_224\n","      *  0.jpg \n","      *  1.jpg...\n","*   B ..."]}]}