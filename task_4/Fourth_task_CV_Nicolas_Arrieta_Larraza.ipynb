{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fourth_task_CV_Nicolas_Arrieta_Larraza.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMA7zhhPKU1HXI631VdnZ/C"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AucZk2sHQ89q"},"source":["# README\n","\n","This delivery contains the implementation of the 4th task of the \"Automatic Signal Detector\" project of the Computer Vision course.\n","In order to run succesfully, one must download the harcascade from \n","\n","*   https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n","\n","Import them in their own drive and change the path in the \"Synchronizing drive files section\"\n","\n","## Important notes:\n","\n","\n","1.   The cells that use the webcam run in a loop, one has to **stop manually the cell** and **re-run the notebook from the next cell** to run the rest of the notebook successfully."]},{"cell_type":"markdown","metadata":{"id":"wsSGUlNySBqt"},"source":["# TASK 4 - MACHINE LEARNING AND COMPUTER VISION PROJECT\n","\n","Author: Nicolás Arrieta Larraza\n","\n","Date: 12/03/2021"]},{"cell_type":"markdown","metadata":{"id":"UoYbeomsSSkg"},"source":["## Initializing"]},{"cell_type":"markdown","metadata":{"id":"3zIBhpYWSVsj"},"source":["### Importing libraries"]},{"cell_type":"code","metadata":{"id":"2P1YxUFGz-jZ","executionInfo":{"status":"ok","timestamp":1615744690602,"user_tz":-60,"elapsed":1007,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import io\n","import cv2 # OpenCV library"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"hzYH3_Zv0-lP","executionInfo":{"status":"ok","timestamp":1615744690604,"user_tz":-60,"elapsed":997,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"4f7f02d8-ad8f-4211-88c7-205dd5035f08"},"source":["cv2.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4.1.2'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"rj9EYEB6SaBX"},"source":["### Synchronizing Drive files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmAJFXZ11SKp","executionInfo":{"status":"ok","timestamp":1615744711970,"user_tz":-60,"elapsed":22358,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"b6fcd940-6dfc-4c17-ad30-440e14d11dc9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xAUO8HAA1Svj","executionInfo":{"status":"ok","timestamp":1615745090352,"user_tz":-60,"elapsed":664,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["# Change for custom location\n","face_cascade_path = \"/content/drive/MyDrive/UCA/ComputerVision/task_1/haarcascade_frontalface_default.xml\"\n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dxou5JMRTe0z"},"source":["### Importing the cascade filter to OpenCV"]},{"cell_type":"code","metadata":{"id":"DG1aNSzj1N5N","executionInfo":{"status":"ok","timestamp":1615745092500,"user_tz":-60,"elapsed":887,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["face_cascades = cv2.CascadeClassifier(face_cascade_path)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-6q96FxStUX"},"source":["## Defining functions"]},{"cell_type":"markdown","metadata":{"id":"VkpGymXDS2qs"},"source":["Function to capture video from computer webcam trough the web browser (given)"]},{"cell_type":"code","metadata":{"id":"G3XFOh6U1IgL","executionInfo":{"status":"ok","timestamp":1615744726081,"user_tz":-60,"elapsed":872,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["from google.colab.output import eval_js\n","\n","def VideoCapture():\n","  js = Javascript('''\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  ''')\n","  display(js)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7OL34_US5W3"},"source":["Functions to convert base64 to bytes array format and viceversa (given). These functions are necessary since the images in Javascript work with base64 format and OpenCV works with bytes array."]},{"cell_type":"code","metadata":{"id":"yOJXhrX41K76","executionInfo":{"status":"ok","timestamp":1615744726328,"user_tz":-60,"elapsed":1113,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["def b64_to_bytes(byte):\n","  jpeg = b64decode(byte.split(',')[1])\n","  im = Image.open(io.BytesIO(jpeg))\n","  return np.array(im)\n","\n","def bytes_to_b64(image):\n","  image = Image.fromarray(image)\n","  buffer = io.BytesIO()\n","  image.save(buffer, 'jpeg')\n","  buffer.seek(0)\n","  x = b64encode(buffer.read()).decode('utf-8')\n","  return x"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-hc2iMTS79u"},"source":["Function to plot histogram of colors"]},{"cell_type":"code","metadata":{"id":"bbhC3TES1NPF","executionInfo":{"status":"ok","timestamp":1615744727218,"user_tz":-60,"elapsed":1997,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["def show_hist(hist):\n","  bin_count = hist.shape[0]\n","  bin_w = 24\n","  img = np.zeros((256, bin_count*bin_w, 3), np.uint8)\n","  for i in range(bin_count):\n","      h = int(hist[i])\n","      cv2.rectangle(img, (i*bin_w+2, 255), ((i+1)*bin_w-2, 255-h), (int(180.0*i/bin_count), 255, 255), -1)\n","  img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n","  cv2_imshow(img)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OHqLRyYMTGjj"},"source":["Function to compute face recognition"]},{"cell_type":"code","metadata":{"id":"xBdlZAaT1oCm","executionInfo":{"status":"ok","timestamp":1615744727220,"user_tz":-60,"elapsed":1994,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["def detect_faces(img, cascades):\n","  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  faces = cascades.detectMultiScale(gray, 1.3, 4)\n","  return faces"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkG2P03Awn6v"},"source":["Function to scale a rotated box by a factor. It is used to scale up the bounding box to remove the noise in the edges of the rectangle."]},{"cell_type":"code","metadata":{"id":"NojYjRfWwlgW","executionInfo":{"status":"ok","timestamp":1615744727221,"user_tz":-60,"elapsed":1991,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}}},"source":["def scale_contour(pts, scale_x, scale_y):\n","    M = cv2.moments(pts)\n","\n","    if M['m00'] == 0:\n","      return pts\n","\n","    cx = int(M['m10']/M['m00'])\n","    cy = int(M['m01']/M['m00'])\n","\n","    cnt_norm = pts - [cx, cy]\n","    cnt_scaled = cnt_norm * np.array([scale_x, scale_y])\n","    cnt_scaled = cnt_scaled + [cx, cy]\n","    cnt_scaled = cnt_scaled.astype(np.int32)\n","\n","    return cnt_scaled"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpgaX6L7TYmN"},"source":["## Capturing region of the recognized faced"]},{"cell_type":"markdown","metadata":{"id":"GbmfgAerXeko"},"source":["First, we need to capture a frame of a face in order to compute it's histogram"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":829},"id":"1ZTTYdvv1qyU","executionInfo":{"status":"ok","timestamp":1615746385137,"user_tz":-60,"elapsed":19325,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"62d528e8-990f-49c3-95a3-246c73ba47a7"},"source":["VideoCapture()\n","eval_js('create()')\n","\n","while True:\n","  response = input(\"Type anything when ready!:\")\n","\n","  # Detecting faces\n","  byte = eval_js('capture()')\n","  im = b64_to_bytes(byte)\n","  im_copy = im.copy()\n","  faces = detect_faces(im, face_cascades)\n","\n","  # Drawing a green rectangle in face's region\n","  if len(faces) == 1:\n","    face = faces[0]\n","    cv2.rectangle(im_copy,(face[0],face[1]),(face[0] + face[2], face[1] + face[3]),(0,255,0),2)\n","  else:\n","    continue\n","  # Showing capture\n","  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im_copy)))\n","  response = input(\"Use this bounding box? [y or n]:\")\n","  # We offer the user the option to keep the previous frame or redo it\n","  if response == 'y':\n","    # In case of accepting we show the selected face frame\n","    frame = im[face[1]:face[1]+face[3], face[0]:face[0]+face[2]]\n","    eval_js('showimg(\"{}\")'.format(bytes_to_b64(frame)))\n","    # Asign it to the tracking window variable that will be use in next steps\n","    tracking_window_face = face\n","    break"],"execution_count":46,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Type anything when ready!:a\n","Use this bounding box? [y or n]:y\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B7J5drGFT0Fb"},"source":["## Building histogram from detected face"]},{"cell_type":"markdown","metadata":{"id":"HGR3ZrGBXob7"},"source":["We compute the histogram of color (only HUE channel) of the face region in order to \"back project\" it in the entire region. Then we check fro every pixel in the image the probability of its color to be part of the histogram. Therefore, the parts of the body with similar colors will show also high probability like arms, hands, etc."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"myNffKnQ1tGG","executionInfo":{"status":"ok","timestamp":1615746389565,"user_tz":-60,"elapsed":777,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"c25af940-ce9c-4f77-d27a-56ac140d0a26"},"source":["# Transform the previous frame into HSV\n","hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","# Creating an inRange mask for pixels\n","mask = cv2.inRange(hsv,np.array((0,64,32)), np.array((180,200,200)))\n","# Compute the histogram of the frame (use only the HUE channel). See `https://bit.ly/3pdVUEd`\n","# Take into account only pixels which are not too bright and not too dark (use the previous mask)\n","# Use 16 bins and speicfy the range of the hue ([0, 180])\n","hist = cv2.calcHist([hsv],[0], mask, [16], [0,180])\n","# Normalize the histogram between 0 (lowest intensity) and 255 (highest intensity) (use MinMax normalization `cv.NORM_MINMAX`) using the method `https://bit.ly/3jMGhCj`\n","hist = cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n","# Reashape the histogram into a 1-D array (use `.reshape(-1)`)\n","hist = hist.reshape(-1)\n","# Show the histogram\n","show_hist(hist)"],"execution_count":47,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAIAAAA1FTYwAAAECklEQVR4nO3dMYoUQRSA4aplLiFG4jHESDyGGInHWPYYYiQeQ4zEY4iReAt7AxkxEcHq8d/Z+b5ko1e86Kd6GbrnAP7gwdhWxr+Pudcm99VVvQBwuQQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgIHP4+WdbO2WuLwJcHjcgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZA71AnACz7al8Y9zpz34CzcgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgILPzl1G3F0vj8/1OewDnwA0IyPg2PPCPtvFqZXyOt25AQEaAgIwAARkBAjICBGQECMgIEJARICDjh4hwNr6MbWX88Zh7bbIXNyAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyBzqBeA377al8Zdzpz34T9yAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjDciwsldj6U3Pd6Me/umRzcgICNAQEaAgIwAARkBAjICBGQECMgIEJDxQ0S4ONv4tDI+x9O9NnEDAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMjc0W/Dbx+WxufznfYATskNCMgIEJARICBzR/8HxJn5sS2NX82d9uDMuAEBGQECMvf8Eezb2pPBw+OTwfXaOTfHcx6tnfP115PK9mTpoPn5eM6btXNeL41z8W4BjoshFvWeVnQAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=RGB size=384x256 at 0x7EFFC52E3210>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"bbk6wyuxUAOs"},"source":["## Erasing face detection to detect just the hand"]},{"cell_type":"markdown","metadata":{"id":"5ech4nwj3yyI"},"source":["In the code below we remove the face detection to focus only on the hand detection. In order to do it we are gonna apply the CAM Shift algorithm to detect the face and erase it by setting probability 0 in the whole bounding box area of the face. Once done, we use CAM Shift again to detect the hand (now it will be the section with more density of probability in the image) and plot a bounding box around it.\n","\n","It is worth mentioning that this system does not work very well with people with beard since the colors of the beard are dominant in the histogram"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RQ3Wn5bvz0yn","executionInfo":{"status":"error","timestamp":1615746493935,"user_tz":-60,"elapsed":21757,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"4184998e-28cb-4a4b-96d3-6b2f94b62042"},"source":["VideoCapture()\n","eval_js('create()')\n","# Stop the mean-shift algorithm iff we effectuated 10 iterations or the computed mean does not change by more than 1pt ~ 1.3px in both directions\n","term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n","\n","byte = eval_js('capture()')\n","im_width, im_height = im.shape[1],im.shape[0]\n","im = b64_to_bytes(byte)\n"," \n","tracking_window_hand = (0,0,im_width,im_height) # Define the initial tracking window for the hand. It spans the entire caption\n","\n","\n","while True:\n","  byte = eval_js('capture()')\n","  im = b64_to_bytes(byte)\n","  # Transform the image into HSV\n","  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","  \n","  # Compute the standard not too bright not too dark mask\n","  mask = cv2.inRange(hsv, np.array((0., 64., 32.)), np.array((180., 200., 200.)))\n","\n","  # Back project the histogramm on the hsv img\n","  prob = cv2.calcBackProject([hsv],[0], hist, [0,180], scale=1)\n","\n","  # Apply the mask\n","  prob = prob & mask\n","\n","  # First look up for the face using cam shift starting from `tracking_window_face`\n","  (x,y,w,h) = tracking_window_face\n","  bbox, tracking_window_face = cv2.CamShift(prob, tracking_window_face, term_crit)\n","\n","  # Retrieve the rotated bounding rectangle\n","  pts = cv2.boxPoints(bbox).astype(np.int)\n","\n","  # Scale the rotated bounding box 1.5x times to eliminate detected parts outside the box \n","  scaled_pts = scale_contour(pts, 1.5, 1.5)\n","\n","  # Fill the rotated face bounding box with 0 in the prob map using`cv2.fillPoly`\n","  cv2.fillPoly(prob, [scaled_pts], 0)\n","\n","  # Draw the boundix box around the face\n","  cv2.polylines(im, [scaled_pts], True, (255, 255 , 0), 2)\n","\n","  # Now look up for the hand using cam shift starting from `tracking_window_hand`\n","  bbox, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n","\n","  pts = cv2.boxPoints(bbox).astype(np.int)\n","\n","  # Scale the contour around the hand\n","  pts = scale_contour(pts, 1.8, 1.5)\n","\n","  ## Draw the boundix box around the hand\n","  cv2.polylines(prob, [pts],True,(255, 255 , 0), 2)\n","  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im)))"],"execution_count":51,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-d9939027d604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Transform the image into HSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}