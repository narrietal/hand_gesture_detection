{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Eighth_task_CV_Nicolas_Arrieta_Larraza.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AucZk2sHQ89q"},"source":["# README\n","\n","This delivery contains the implementation of the 8th task of the \"Automatic Signal Detector\" project of the Computer Vision course.\n","\n","In this notebook we test the models trained in tasks 7 and 9. The current version of the model is trained to detect the letters \"A\", \"B\" and \"C\". For more information please check following [sign language alphabet](https://en.wikipedia.org/wiki/Fingerspelling#/media/File:Asl-sign-language-coloring-at-coloring-pages-for-kids-boys-dotcom.svg)\n","\n","The last cell of the notebook needs of the following interaction with the user:\n","\n","\n","1.   Firstly, it will ask the user to choose between the model trained in task 7 (**cnn**) or the model trained in task 9 (**vgg**)\n","\n","2.   Sencondly, the user must show his hand on camera and the script will ask to **confirm if the bounding box correctly detected the hand**.\n","\n","3.  Once it is confirmed, the script will show the predicted letter **in screen**.\n","\n","In order to run succesfully, one must download the harcascade from \n","\n","*   https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n","\n","Import them in their own drive and change the path in the \"Synchronizing drive files section\"\n","\n","## Important notes:\n","\n","1.   The cells that use the webcam run in a loop, one has to **stop manually the cell** and **re-run the notebook from the next cell** to run the rest of the notebook successfully."]},{"cell_type":"markdown","metadata":{"id":"wsSGUlNySBqt"},"source":["# TASK 8 - MACHINE LEARNING AND COMPUTER VISION PROJECT\n","\n","Author: Nicolás Arrieta Larraza\n","\n","Date: 03/05/2021"]},{"cell_type":"markdown","metadata":{"id":"UoYbeomsSSkg"},"source":["## Initializing"]},{"cell_type":"markdown","metadata":{"id":"3zIBhpYWSVsj"},"source":["### Importing libraries"]},{"cell_type":"code","metadata":{"id":"2P1YxUFGz-jZ"},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import io\n","import cv2 # OpenCV library\n","from pathlib import Path\n","import os\n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"hzYH3_Zv0-lP","executionInfo":{"status":"ok","timestamp":1619720013660,"user_tz":-120,"elapsed":2948,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"07fcad30-fd9d-43ea-c6cb-580170e80460"},"source":["cv2.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4.1.2'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"rj9EYEB6SaBX"},"source":["### Synchronizing Drive files"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmAJFXZ11SKp","executionInfo":{"status":"ok","timestamp":1619720075378,"user_tz":-120,"elapsed":64654,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"18660cdf-083a-4a0b-f031-6f3bf02b1e24"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N7ee93FrIZ6D"},"source":["Specifying face cascade's path"]},{"cell_type":"code","metadata":{"id":"xAUO8HAA1Svj"},"source":["# Change for custom location\n","face_cascade_path = \"/content/drive/MyDrive/UCA/ComputerVision/task_1/haarcascade_frontalface_default.xml\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qgTT71RkI3JC"},"source":["Specifying the paths of the models from task 7 and 9"]},{"cell_type":"code","metadata":{"id":"5iltqq5QIkcH"},"source":["cnn_model_path = \"/content/drive/MyDrive/UCA/ComputerVision/task_7/custom_model.h5\"\n","vgg_model_path = \"/content/drive/MyDrive/UCA/ComputerVision/task_9/vgg_model.h5\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dxou5JMRTe0z"},"source":["### Importing the cascade filter to OpenCV"]},{"cell_type":"code","metadata":{"id":"DG1aNSzj1N5N"},"source":["face_cascades = cv2.CascadeClassifier(face_cascade_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-6q96FxStUX"},"source":["## Defining functions"]},{"cell_type":"markdown","metadata":{"id":"VkpGymXDS2qs"},"source":["Function to capture video from computer webcam trough the web browser (given)"]},{"cell_type":"code","metadata":{"id":"G3XFOh6U1IgL"},"source":["from google.colab.output import eval_js\n","\n","def VideoCapture():\n","  js = Javascript('''\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  ''')\n","  display(js)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7OL34_US5W3"},"source":["Functions to convert base64 to bytes array format and viceversa (given). These functions are necessary since the images in Javascript work with base64 format and OpenCV works with bytes array."]},{"cell_type":"code","metadata":{"id":"yOJXhrX41K76"},"source":["def b64_to_bytes(byte):\n","  jpeg = b64decode(byte.split(',')[1])\n","  im = Image.open(io.BytesIO(jpeg))\n","  return np.array(im)\n","\n","def bytes_to_b64(image):\n","  image = Image.fromarray(image)\n","  buffer = io.BytesIO()\n","  image.save(buffer, 'jpeg')\n","  buffer.seek(0)\n","  x = b64encode(buffer.read()).decode('utf-8')\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-hc2iMTS79u"},"source":["Function to plot histogram of colors"]},{"cell_type":"code","metadata":{"id":"bbhC3TES1NPF"},"source":["def show_hist(hist):\n","  bin_count = hist.shape[0]\n","  bin_w = 24\n","  img = np.zeros((256, bin_count*bin_w, 3), np.uint8)\n","  for i in range(bin_count):\n","      h = int(hist[i])\n","      cv2.rectangle(img, (i*bin_w+2, 255), ((i+1)*bin_w-2, 255-h), (int(180.0*i/bin_count), 255, 255), -1)\n","  img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n","  cv2_imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OHqLRyYMTGjj"},"source":["Function to compute face recognition"]},{"cell_type":"code","metadata":{"id":"xBdlZAaT1oCm"},"source":["def detect_faces(img, cascades):\n","  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  faces = cascades.detectMultiScale(gray, 1.3, 4)\n","  return faces"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkG2P03Awn6v"},"source":["Function to scale a rotated box by a factor. It is used to scale up the bounding box to remove the noise in the edges of the rectangle."]},{"cell_type":"code","metadata":{"id":"NojYjRfWwlgW"},"source":["def scale_contour(pts, scale_x, scale_y):\n","    M = cv2.moments(pts)\n","\n","    if M['m00'] == 0:\n","      return pts\n","\n","    cx = int(M['m10']/M['m00'])\n","    cy = int(M['m01']/M['m00'])\n","\n","    cnt_norm = pts - [cx, cy]\n","    cnt_scaled = cnt_norm * np.array([scale_x, scale_y])\n","    cnt_scaled = cnt_scaled + [cx, cy]\n","    cnt_scaled = cnt_scaled.astype(np.int32)\n","\n","    return cnt_scaled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24l3DKy4n5WC"},"source":["Function to create box arounf CAM Shift resulting box\n"]},{"cell_type":"code","metadata":{"id":"TI5ajMl_n48Q"},"source":["def get_hand_box(pts, im_w, im_h):\n","  x_top_l = max(0, min(pts[:,0]))\n","  y_top_l = max(0, min(pts[:,1]))\n","  x_bottom_r = min(im_w, max(pts[:,0]))\n","  y_bottom_r = min(im_h,  max(pts[:,1 ]))\n","  return (x_top_l, y_top_l), (x_bottom_r, y_bottom_r)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpgaX6L7TYmN"},"source":["## Capturing region of the recognized faced"]},{"cell_type":"markdown","metadata":{"id":"GbmfgAerXeko"},"source":["First, we need to capture a frame of a face in order to compute it's histogram"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735},"id":"1ZTTYdvv1qyU","executionInfo":{"status":"ok","timestamp":1619720205812,"user_tz":-120,"elapsed":25072,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"4bb4a027-2635-4111-93d5-7fd02017580b"},"source":["VideoCapture()\n","eval_js('create()')\n","\n","while True:\n","  response = input(\"Type anything when ready!:\")\n","\n","  # Detecting faces\n","  byte = eval_js('capture()')\n","  im = b64_to_bytes(byte)\n","  im_copy = im.copy()\n","  faces = detect_faces(im, face_cascades)\n","\n","  # Drawing a green rectangle in face's region\n","  if len(faces) == 1:\n","    face = faces[0]\n","    cv2.rectangle(im_copy,(face[0],face[1]),(face[0] + face[2], face[1] + face[3]),(0,255,0),2)\n","  else:\n","    continue\n","  # Showing capture\n","  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im_copy)))\n","  response = input(\"Use this bounding box? [y or n]:\")\n","  # We offer the user the option to keep the previous frame or redo it\n","  if response == 'y':\n","    # In case of accepting we show the selected face frame\n","    frame = im[face[1]:face[1]+face[3], face[0]:face[0]+face[2]]\n","    eval_js('showimg(\"{}\")'.format(bytes_to_b64(frame)))\n","    # Asign it to the tracking window variable that will be use in next steps\n","    tracking_window_face = face\n","    break\n","\n","im = im"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Type anything when ready!:f\n","Use this bounding box? [y or n]:y\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B7J5drGFT0Fb"},"source":["## Building histogram from detected face"]},{"cell_type":"markdown","metadata":{"id":"HGR3ZrGBXob7"},"source":["We compute the histogram of color (only HUE channel) of the face region in order to \"back project\" it in the entire region. Then we check fro every pixel in the image the probability of its color to be part of the histogram. Therefore, the parts of the body with similar colors will show also high probability like arms, hands, etc."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"myNffKnQ1tGG","executionInfo":{"status":"ok","timestamp":1619720213595,"user_tz":-120,"elapsed":811,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"afc9d186-428d-4a9f-f276-de1bb5784c1b"},"source":["# Transform the previous frame into HSV\n","hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","# Creating an inRange mask for pixels\n","mask = cv2.inRange(hsv,np.array((0,64,32)), np.array((180,200,200)))\n","# Compute the histogram of the frame (use only the HUE channel). See `https://bit.ly/3pdVUEd`\n","# Take into account only pixels which are not too bright and not too dark (use the previous mask)\n","# Use 16 bins and speicfy the range of the hue ([0, 180])\n","hist = cv2.calcHist([hsv],[0], mask, [16], [0,180])\n","# Normalize the histogram between 0 (lowest intensity) and 255 (highest intensity) (use MinMax normalization `cv.NORM_MINMAX`) using the method `https://bit.ly/3jMGhCj`\n","hist = cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n","# Reashape the histogram into a 1-D array (use `.reshape(-1)`)\n","hist = hist.reshape(-1)\n","# Show the histogram\n","show_hist(hist)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAIAAAA1FTYwAAAD9UlEQVR4nO3aMWoVQRzA4dnwLiFWkmOIVfAYYiUeI+QYYiUeI1iJxxAr8RauTTakM2QWfs/1+6pt5s+85sfMY5YBx3O1Ti3/suy0D/7iot4A8P8SICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZE71BuB8PRvrzPJfY9lrJ0flBARkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyp3oDcHzXY51ZfjOWvXZybpyAgIwAARkBAjICBGQECMgIEJARICAjQEDGQ0TOyaepB3vj7WEf7B2VExCQESAgI0BARoCAjAABGQECMgIEZAQIyOz8EHF9M7V8+bzTPoB/gZfQwBOt493M8mV8PG2DpngADzyB/4CAzJlewdbbqeXL67uPn3NHu+fb0e56bs7NNufF3Jwf90fN9eXUoOXbNufD3Jz3dx+/537YhTP0o3yfu6tcbneVdXydmbOMVzPLH/oDXr0hDigDrvUAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=RGB size=384x256 at 0x7FBA9FB09E50>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"qIdTmuLwGz2U"},"source":["## Importing models\n"]},{"cell_type":"markdown","metadata":{"id":"-LEN_LLtHkrc"},"source":["For the classifier we can use choose between the model trained in task 7 or task 9"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8yU2WVZuGoPa","executionInfo":{"status":"ok","timestamp":1619720220806,"user_tz":-120,"elapsed":1950,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"e99ef672-516e-4fec-9a14-d7443738c56c"},"source":["# Import trained model from task 7\n","cnn_model = tf.keras.models.load_model(cnn_model_path)\n","\n","# Show the model architecture\n","cnn_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 32, 32, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 2, 2, 128)         0         \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                8256      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 138,051\n","Trainable params: 138,051\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Vtcjuh3Jid7","executionInfo":{"status":"ok","timestamp":1619720230870,"user_tz":-120,"elapsed":10179,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"94ebbeda-0779-4f9f-8f30-a8bb34db245e"},"source":["# Import trained model from task 9\n","vgg_model = tf.keras.models.load_model(vgg_model_path)\n","\n","# Show the model architecture\n","vgg_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","feature_extractor (Functiona (None, 4096)              139570240 \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1024)              4195328   \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 3)                 3075      \n","=================================================================\n","Total params: 143,768,643\n","Trainable params: 4,198,403\n","Non-trainable params: 139,570,240\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bbk6wyuxUAOs"},"source":["## Erasing face detection, detecting hand and classifying letter"]},{"cell_type":"markdown","metadata":{"id":"5ech4nwj3yyI"},"source":["In the code below we remove the face detection to focus only on the hand detection. In order to do it we are gonna apply the CAM Shift algorithm to detect the face and erase it by setting probability 0 in the whole bounding box area of the face. Once done, we use CAM Shift again to detect the hand (now it will be the section with more density of probability in the image) and plot a bounding box around it.\n","\n","Once we detect the hand, we are going to use the images as input data for our trained model to detect the letter.\n","\n","As mentioned in the README section, the script needs of the following interaction with the user:\n","\n","\n","1.   Firstly, it will ask the user to choose between the model trained in task 7 (**cnn**) or the model trained in task 9 (**vgg**)\n","\n","2.   Sencondly, the user must show his hand on camera and the script will ask to **confirm if the bounding box correctly detected the hand**.\n","\n","3.  Once it is confirmed, the script will show the predicted letter **in screen**.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RQ3Wn5bvz0yn","executionInfo":{"status":"error","timestamp":1619720805793,"user_tz":-120,"elapsed":107924,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"cb9dd0ec-bb00-4d1d-9384-4a795d459510"},"source":["\n","VideoCapture()\n","eval_js('create()')\n","# Stop the mean-shift algorithm iff we effectuated 10 iterations or the computed mean does not change by more than 1pt ~ 1.3px in both directions\n","term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n","\n","byte = eval_js('capture()')\n","im_width, im_height = im.shape[1],im.shape[0]\n","im = b64_to_bytes(byte)\n","\n","tracking_window_hand = (0,0,im_width,im_height) # Define the initial tracking window for the hand. It spans the entire caption\n","already_defined_box = False # Auxiliar variable that checks if the hand box is well defined\n","done_A, done_B, done_C = False, False, False # Auxiliar variables to manage the creation of the dataset\n","img_count = 0 # Auxiliar variable to count the images created\n","\n","img_dim = 32 # Final dimension of image\n","gray_scale = True # Gray scale image\n","channels = 1\n","selected_model = False\n","model = cnn_model\n","\n","while True:\n","\n","  if not selected_model:\n","    res = input(\"PLEASE, SELECT THE MODEL TO TEST [cnn or vgg]:\")\n","    if res == \"cnn\":\n","      img_dim = 32\n","      gray_scale = True\n","      channels = 1\n","      model = cnn_model\n","      selected_model = True\n","\n","    elif res == \"vgg\":\n","      img_dim= 224\n","      gray_scale = False\n","      channels = 3\n","      model = vgg_model\n","      selected_model = True\n","    \n","    else:\n","      print(\"PLEASE, SELECT A VALID MODEL\")\n","      selected_model = False\n","  \n","  else:\n","    byte = eval_js('capture()')\n","    im = b64_to_bytes(byte)\n","    # Transform the image into HSV\n","    hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","    \n","    # Compute the standard not too bright not too dark mask\n","    mask = cv2.inRange(hsv, np.array((0., 64., 32.)), np.array((180., 200., 200.)))\n","\n","    # Back project the histogramm on the hsv img\n","    prob = cv2.calcBackProject([hsv],[0], hist, [0,180], scale=1)\n","\n","    # Apply the mask\n","    prob = prob & mask\n","\n","    # First look up for the face using cam shift starting from `tracking_window_face`\n","    (x,y,w,h) = tracking_window_face\n","    bbox, tracking_window_face = cv2.CamShift(prob, tracking_window_face, term_crit)\n","\n","    # Retrieve the rotated bounding rectangle\n","    pts = cv2.boxPoints(bbox).astype(np.int)\n","\n","    # Scale the rotated bounding box 1.5x times to eliminate detected parts outside the box \n","    scaled_pts = scale_contour(pts, 1.5, 1.5)\n","\n","    # Fill the rotated face bounding box with 0 in the prob map using`cv2.fillPoly`\n","    cv2.fillPoly(prob, [scaled_pts], 0)\n","\n","    # Draw the boundix box around the face\n","    cv2.polylines(im, [scaled_pts], True, (255, 255 , 0), 2)\n","\n","    # Now look up for the hand using cam shift starting from `tracking_window_hand`\n","    bbox, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n","\n","    pts = cv2.boxPoints(bbox).astype(np.int)\n","\n","    # Scale the contour around the hand\n","    pts = scale_contour(pts, 1.5, 1.5)\n","\n","    #Creating box around cam shift leaned box\n","    hand_box = get_hand_box(pts, im_width, im_height)\n","\n","    cv2.rectangle(im, hand_box[0], hand_box[1], (255,255,0), 2)\n","\n","    #Checks if the hand box is in the right positiom before taking samples, needs human interaction\n","    if not already_defined_box:\n","      eval_js('showimg(\"{}\")'.format(bytes_to_b64(im)))\n","      response = input(\"Is the hand box right positioned? [y or n]:\")\n","      if response == 'y':\n","        already_defined_box = True\n","    \n","    else:\n","      if gray_scale:\n","        #Converting image into black and white\n","        hand = cv2.cvtColor(im[hand_box[0][1]:hand_box[1][1], hand_box[0][0]:hand_box[1][0]], cv2.COLOR_BGR2GRAY)\n","      else:\n","        hand = im[hand_box[0][1]:hand_box[1][1], hand_box[0][0]:hand_box[1][0]]\n","\n","      #Resizing image\n","      hand = cv2.resize(hand, (img_dim,img_dim))\n","      hand_input_format = hand.reshape(-1,img_dim,img_dim,channels)\n","\n","      prediction = model.predict(hand_input_format)\n","      prediction = prediction.argmax()\n","      predicted_letter = chr(ord('A') + prediction)\n","\n","      font = cv2.FONT_HERSHEY_SIMPLEX\n","      cv2.putText(im,predicted_letter,(15,425), font, 2,(0,255,0),3)\n","\n","    eval_js('showimg(\"{}\")'.format(bytes_to_b64(im)))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["PLEASE, SELECT THE MODEL TO TEST [cnn or vgg]:cnn\n","Is the hand box right positioned? [y or n]:n\n","Is the hand box right positioned? [y or n]:cnn\n","Is the hand box right positioned? [y or n]:y\n"],"name":"stdout"},{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-dff2c5ce81b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgray_scale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#Converting image into black and white\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mhand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhand_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]}]}